Barra - NVidia G80 functional simulator
 Version 0.1
 Sylvain Collange (sylvain.collange @ univ-perp.fr), 2009

Barra simulates CUDA programs at the assembly language level (Tesla ISA). Its
goal is to be 100% bit-accurate, offering the exact same behavior as NVidia
hardware. It works directly on the executable; no modification nor
recompilation is required.

It is primarily intended as a tool for research on hardware architecture, although
it can also be used to debug, profile and optimize CUDA programs at the lowest
level.

Supported platforms:
Barra was tested on GNU/Linux i386 and x86_64. It should be easily portable to
Win32 with Cygwin and Mac OS X, but this has not been tested so far.


  How Does It Work?

Barra is comprised of a simulator and a driver.
The Barra driver is a dynamic library which exports the same symbols and API
as the CUDA Driver library (libcudart.so/cudart.dll).

To simulate a CUDA program which uses the Driver API, we temporarily replace
the NVidia-provided CUDA Driver library with our Barra library by setting
the LD_LIBRARY_PATH or PATH variable. This way, cuXxx calls are redirected
to the Barra driver, which can then configure and run the simulator as if
it were an actual GPU. When cuLaunch or cuLaunchGrid is called, control is
transferred to the simulator for execution.

Programs that use the CUDA Runtime API are still linked with the official CUDA
Runtime library provided by NVidia (libcuda.so/cuda.dll).
This Runtime library is only a wrapper over the Driver library, which
translates cudaXxx calls into cuYyy calls.
We can then trick the Runtime library into using our driver instead of NVidia's
driver, just as we do with programs using the Driver API directly.

Launch helper script pending...


  What Is Supported?

Simulator
* Most integer arithmetic, floating-point arithmetic, logical instructions;
* Memory scatter/gather instructions from/to global and local memory up to
  32-bit;
* Basic control flow instructions;
* Reciprocal, reciprocal square root and transcendental instructions (not
  bit-accurate).

Driver
* Most of the CUDA Driver API;
* CUDA runtime API, through NVidia-provided libcuda.so/cuda.dll;
* Support for cubin files and Fat Executables (through CUDA Runtime);
* Host <-> Device linear memory copy.


  What Runs On Barra?

The following NVidia CUDA SDK samples were tested to run on Barra and pass the
built-in regression tests:
* bitonic
* deviceQuery
* dwtHaar1D
* fastWalshTransform
* histogram64
* histogram256
* matrixMul
* matrixMulDrv
* MersenneTwister
* MonteCarlo
* reduction
* scan
* scanLargeArray
* simpleTemplates
* transpose

Note: for some samples, the size of the dataset was reduced to keep simulation
time reasonable (as with device emulation mode).


  What Is NOT Currently Supported? (Aka TODO List)

Simulator
* Some conversion and comparison instructions;
* 64-bit and 128-bit memory access;
* Exotic control flow instructions: break, continue;
* Atomic instructions;
* Warp vote instructions;
* Double precision instructions;
* Texture sampling;
* Multiple Streaming Multiprocessors (after adding TLM timing, see below);
* Bit-accurate transcendentals;
* Run-time checks. As Barra is primarily designed to run valid CUDA benchmarks,
  few safety checks are performed on instruction validity, memory addresses...
  Running an invalid CUDA program is most likely to result in a segmentation
  fault.

Driver
* Asynchronous execution;
* Textures;
* Arrays;
* Events;


  How Fast (Or How Slow) Is It?

In most SDK samples, Barra performs 3 to 10 times slower than source-level
emulation in debug mode (nvcc --deviceemu), which is itself several orders
of magnitude slower than execution on a high-end GPU.


  What We Plan To Do Next

* Transaction-Level Modeling of the G80 memory architecture to provide a
  realistic timing model.
* Cycle-accurate modeling of the Streaming Multiprocessor.
* Modeling of power consumption.
* Simulation speed optimizations.


  Feedback
  
Bug reports and comments are welcome. You can contact me at
sylvain.collange @ univ-perp.fr


  Special Thanks

* Wladimir J. van der Laan, for his amazing work on recovering the G80
  instruction set and for developing the decuda and cudasm tools, without
  which this work would not have been possible.

