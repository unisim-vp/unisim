Barra - NVidia G80 functional simulator
 Version 0.1
 04/2009

Barra simulates CUDA programs at the assembly language level (Tesla ISA). Its
ultimate goal is to provide a 100% bit-accurate simulation, offering
bug-for-bug compatibility with NVidia G80-based GPUs. It works directly with
CUDA executables; no source modification nor recompilation is required. 

Barra is primarily intended as a tool for research on computer architecture,
although it can also be used to debug, profile and optimize CUDA programs at
the lowest level.

Publication:
Sylvain Collange, David Defour, David Parello.
Barra, a Modular Functional GPU Simulator for GPGPU.
Technical Report hal-00359342, UniversitÃ© de Perpignan, 2009.
http://hal.archives-ouvertes.fr/hal-00359342

Download:
Barra source code can be accessed through the Unisim subversion repository at
http://unisim.org/site/download/start#subversion_repository

Supported platforms:
Barra was tested on GNU/Linux i386 and x86_64. It should be easily portable to
Win32 with Cygwin and Mac OS X, but this has not been tested so far.

CUDA 2.0 and CUDA 2.1 are supported. 

  How Does It Work?

Barra is comprised of a simulator and a driver.
The Barra driver is a dynamic library which exports the same symbols and API
as the CUDA Driver library (libcuda.so/cuda.dll).

To simulate a CUDA program which uses the Driver API, we temporarily replace
the NVidia-provided CUDA Driver library with our Barra library by setting
the LD_LIBRARY_PATH or PATH variable. This way, cuXxx calls are redirected
to the Barra driver, which can then configure and run the simulator as if
it were an actual GPU. When cuLaunch or cuLaunchGrid is called, control is
transferred to the simulator for execution.

Programs that use the CUDA Runtime API are still linked with the official CUDA
Runtime library provided by NVidia (libcudart.so/cudart.dll).
This Runtime library is only a wrapper over the Driver library, which
translates cudaXxx calls into cuYyy calls.
We can then trick the Runtime library into using our driver instead of NVidia's
driver, just as we do with programs using the Driver API directly.


  How To Use It?

First, compile and install unisim-tools, unisim-lib and unisim-simulators
according to each INSTALL instructions.

Let's assume we want to simulate the matrixMul sample of the NVidia CUDA SDK
under GNU/Linux.
The Barra driver is named libunisimsimulator_cxx_tesla_tesla.so (this might
change in the future. libbarra.so or even libcuda.so are good candidates).
Since we want to override libcuda.so, we create some symbolic links to our
library. Assuming Unisim was installed in /usr/local/unisim:
> cd NVIDIA_CUDA_SDK/bin/linux/debug
> ln -s /usr/local/unisim/lib/libunisimsimulator_cxx_tesla_tesla.so libcuda.so.1
> ln -s libcuda.so.1 libcuda.so
(Note: we need to create both files, as the CUDA Runtime looks for libcuda.so.1,
while most programs using the CUDA Driver API will look for libcuda.so.)

We then temporarily override the default library search path:
> LD_LIBRARY_PATH=".:/usr/local/unisim/lib/"

Finally, we can launch our executable:
> ./matrixMul

Barra currently outputs lots of debug information (CUDA function calls,
cubin data, disassembly, memory allocation, thread scheduling...) during
program execution. 

We are working on a script that will automate these manipulations.


  What Is Supported?

Simulator
* Most integer arithmetic, floating-point arithmetic, logical instructions.
* Memory scatter/gather instructions from/to global and local memory up to
  32-bit.
* Basic control flow instructions.
* Reciprocal, reciprocal square root and transcendental instructions (not
  bit-accurate).
* Synchronization barrier instruction.

Driver
* Most of the CUDA Driver API.
* CUDA runtime API, through NVidia-provided libcudart.so/cudart.dll.
* Support for cubin files and Fat Executables (through CUDA Runtime).
* Host <-> Device linear memory copy.


  What Runs On Barra?

The following NVidia CUDA SDK samples were tested to run on Barra and pass the
built-in regression tests:
* alignedTypes
* bitonic
* binomialOptions
* convolutionSeparable
* deviceQuery
* dwtHaar1D
* fastWalshTransform
* histogram64
* histogram256
* matrixMul
* matrixMulDrv
* MersenneTwister
* MonteCarlo
* quasirandomGenerator
* reduction
* scan
* scanLargeArray
* simpleTemplates
* transpose

Note: for some samples, the size of the dataset was reduced to keep simulation
time reasonable (as with device emulation mode).


  What Is NOT Currently Supported? (Aka TODO List)

Simulator
* Some conversion and comparison instructions.
* 64-bit and 128-bit memory access.
* Exotic control flow instructions: break, continue.
* Atomic instructions.
* Warp vote instructions.
* Double precision instructions.
* Texture sampling.
* Multiple Streaming Multiprocessors (after adding TLM timing, see below).
* Bit-accurate transcendentals.
* Run-time checks. As Barra is primarily designed to run valid CUDA benchmarks,
  few safety checks are performed on instruction validity, memory addresses...
  Running an invalid or buggy CUDA program is likely to result in a
  segmentation fault.

Driver
* Asynchronous execution.
* Textures.
* Arrays.
* Events.
* Multiple contexts.
* Multiple devices.

  How Fast (Or How Slow) Is It?

Depending on the simulated code, Barra performs from 10 times slower to 10
times faster than source-level emulation in debug mode (nvcc --deviceemu),
which is itself several orders of magnitude slower than execution on a
high-end GPU. 

  What We Plan To Do Next

* Instrumentation to gather statistics about the CUDA code.
* Transaction-Level Modeling of the G80 memory architecture to provide a
  realistic timing model.
* (Close to) cycle-accurate modeling of Streaming Multiprocessors.
* Modeling of power consumption.
* Simulation speed optimization.


  Hacking, debugging

As a computer architecture research tool, Barra is designed to be modified
to suit the user's needs (e.g. gathering statistics on instructions, generating
traces...)

Some support is already present to generate traces. It is intended to be used
for manual debugging and trace format may change without notice.
Several environment variables control the level of verbosity of traces. 

* TRACE_INSN outputs to stderr each instruction executed along with the warp
  number and program counter.
  All subsequent trace types are designed to be used with TRACE_INSN.
* TRACE_MASK outputs the current predication mask of the warp after (not during)
  each instruction.
* TRACE_REG outputs the destination register of each instruction executed,
  in hex.
* TRACE_REG_FLOAT. Same as TRACE_REG, but as floating-point. Requires TRACE_REG.
* TRACE_LOADSTORE. *Very* verbose. Traces every load and store from/to any memory
  type.
* TRACE_BRANCH controls the output of the SIMD branching algorithms.
* TRACE_SYNC outputs which warps are waiting at synchronization barriers.


  Feedback
  
Contact: sylvain.collange @ univ-perp.fr
Bug reports and comments are welcome.

  Credits

* Sylvain Collange (sylvain.collange @ univ-perp.fr), developer
* Marc Daumas (marc.daumas @ univ-perp.fr)
* David Defour (david.defour @ univ-perp.fr)
* David Parello (david.parello @ univ-perp.fr)


  Special Thanks

* Wladimir J. van der Laan, for his amazing work on recovering the G80
  instruction set and for developing the decuda and cudasm tools, without
  which this work would not have been possible.

